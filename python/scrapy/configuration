所有操作：
请求 scrapy.Request, scrapy.follow(有join功能）
settings: 对应表的名字和spider的名字， 表要事先创建好
middleware， download的，主要目的是针对名字是google的那个设定proxy，
	使用privoxy设置，本地转sock到http
pipeline
	imagepipeline：
		get media request 这个返回request的列表，可以设置meta，这个用来标定type，如果用class的属性
		会产生错乱，用这个好, 在response里可以查到
		item completed 根据图片下载完成的的情况进行赋值
		file path  这个用来产生特定路径
	store直接存储
		使用from crawler class方法开始，

各种匹配可以用lxml。html。fromstring, re   或者直接的response。xpath， 
	有些不好解析，会用到//div[contains(@class,'pattern')]
	有时候urlopen比requests有用，只有他能解析出来，
	使用urllib.parse.urljoin 连接
		urllib.request.Request 来设置url 和header
		urllib.request.urlopen 打开上面的request对象，然后read().decode('utf-8')就可以得到页面了

post:
	scrapy.FormRequest,
	param:
		formdata 字典，
		不需要写method=post，自动就是的
		一般url是一样的，所以要设置，dont_filter=True防止过滤

设置更改：
	每个spider可以自己设定，在custom_settings = {xx:xx }可以这样在类里面设置	
	DOWNLOAD_TIMEOUT = 40 这个可以更改，是超时时间，时常注意

文档：
	这个要注意，最好和当前版本一样，或者找左上角lastest的版本，英文为好

scrapy-redis:
	这个有个pipeline，不一定要加的，加了之后item会添加一个回到redis队列里面去，用的rpush，lpop这个要注意，这样才顺序


RedisSpider: make_request_from_data 方法重写，可以操控rediskey里的数据
	自己实现到底get post 或者组合非url, 每次调用应该返回一个Request类
	在里面使用scrapy.FormRequest 形成post 见 https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks

redirect:
	302 在scrapy中的重定向会把post改为get，这个会在algaebase网站出现问题，
	scrapy/downloadermiddlewares/redirect.py 里面进行一些修改，把302加入不修改method的重定向即可

关于parse的返回：
	这个函数必须本身要有返回，request或者item or None，如果调用的子函数来进行这些操作的，那么就
	用for res in subfunc（xxx）：
		yield res 这样来返回
