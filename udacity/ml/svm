向量机就是把两类数据分离，找到能分离的最佳线，距离两边距离最长。
	可以归结为2次规划，拉格朗日乘子算法，然后里面大多数系数都是0，
	因为大多数两边的点对svm的线没有影响，只有最近的几个有影响，
	然后两个点的线性程度越大，影响越大，最后的最大值越小

两个向量的点积还可以表现为他们的相似度，这个用在高纬度里面，
	可以用圆来分割，表示在圆内还是外

这种相似度的概念，其实和K邻近的概念是一样的，
	越是相似，相互作用的值越偏向一个域，这样就会被归为一类，
	如果不相似，归为另一类

核函数：(域知识）
	所有以上的这种任意两个向量的关系，包括其他利用这种关系的函数，
	都是核函数
这个概念可以利用到很多对象上面，不一定是同空间的向量
	可以把这个看作是在已有的纬度上的一个新纬度，区分了所有的点
	中间用超平面隔开,可以有多种分类函数，其实就是一个利用
		原来向量作运算，然后分出间隔的函数

	高维的线性可以是低纬度的非线性 ，利用核函数
mercer condition

优点：
	增强分类的健壮性，因为这样分类不容易被破坏，距离两边最远
	这个距离叫margin!!

参数：
	会影响形状的三个参数：
	kernel, C, gamma
	C 表示惩罚程度，越大表示越希望所有的都正确，就不一定是直线了
	gamma是参数具体效果还没测量

	注意overfitting

缺点：
	容易变得复杂，过 拟合，而且训练耗时
		存在过多噪音

原理： ref: http://blog.pluskid.org/?p=632
	对于所有点，我们假设有一个超平面可以分开这些点
	那么平面上=0, 以上>0, 以下<0, 我们把对应的ｙ值分别设为１，－１这个主要用来分类，值不重要
	然后我们所要达到的，就是找到一个平面，距离两类点的距离中最小的距离要最大化，同时还要分类正确

	因为平面法向量就是参数向量（可以用平面内两个点相减的式子的点乘看出来）
	然后找到平面外一点到平面内对应的垂直点，然后利用公式得到距离的公式，再乘以ｙ去掉符号，
	这样得到的就是距离了，
	而我们就是要找到这样的距离中最短的那个的最大值，
	for (xi,yi) in all:
		yi(xi*w + b) >= (y_min*||w||),  y_min = y_dis / ||w||,这里固定y_dis　因为这个和ｗ可以等比例增长，
		所以固定后，可以屏蔽不必要的靠ｖｌ因素，球出来后再等比例涨也无妨,就是相当于先找到他们的比例，
		这个才是实质需要的
	我们要求的就是y_min的最大值，也就是||w||的最小值

	在这里讨论的时候，我们固定了最短函数距离为１，几何距离可以随ｗ而变动，这个就是要求，放着
		这样，我们可以确定支持边界上的点满足y(wx+b) = 1
		这样就变成了求||w||最小值，y(x*w+b) > 1了（因为固定函数距离是１了）

	这里利用 0.5*||w||^2　构造二次规划，可以用二次规划的方法解答，
			同时也可以使用拉格朗日乘子法，把条件也放到目标式子里面，然后
			找到对偶的函数，满足ｋｔｔ条件，对偶的解就是本体的解，这个容易一点，
			然后求偏导，找到极值点
		

核函数
	这个看对偶式子里的点积项，在非线性可分的时候，利用映射，可以找到高维度的可以分正负的式子，
	利用那个式子的每项做一个维度，可以分割的，但是这个超高维度，不现实，然而可以在对偶最终计算出来的
	式子里找到ｗ的替换式子，里面实际上区分的时候利用的点乘的元素，可以直接在当前维度利用增加一维的方法
	获得新区分值，这就是ｋｅｒｎｅｌｔｒｉｃｋ，
	不同的核函数：
		多项式： (<x1,x2>+R)^d
		高斯核：exp(-(||x1-x2||^2 // 2theta^2)
			这个利用ｔｈｅｔａ的不同值，可以映射到不同的维度，一定可分，单要注意过度拟合
		当然，每个ｗ参数都有一个系数的，这个要利用对偶求解，所以这个核函数不是瞎猜就可以搞定的
