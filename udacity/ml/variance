pca降维度：
	就是把ｎ维向量映射到ｍ维，所谓的映射，其实就是
	ｎ维的向量，对着ｍ维的基进行映射，所有基上的映射相加就行了．

	为了让这样的映射提取效果最好：
		各个维度内部自己的方差，这个要最大，也就是加起来要最大？
			这个理解为映射到一个基上之后，各个数据之间的离散程度
		任意选择两组基，他们之间的相关度要最小，也就是任意两个维度之间
		所有的向量之间的点乘要为０,这样说明映射到的两个基是垂直的？

		协方差中每一项就是所有样本里的固定两个特征相乘的相加，构成矩阵的一项
			所以以矩阵的角度看就是任意选择两列，纵向每行对应相乘，加起来
			这样求出来的，其实就是两条特征之间的相关度的一个度量值，
			为０的话意思就是两个维度向量垂直，就是一个向量投影到另一个上面
			结果为０，毫无影响，完全无关（还是看矩阵分析吧．．．．）

ica:
	独立成分分析：
	用于盲源问题，可以把信息中的互相之间没有信息量的成份取出，
		在图片中，可以理解为确定边界
rca:
	随机成分分析：
		这个和ｐｃａ类似但是因为快，所有优势，缺点是可能没有那么精简，效果也没有那么好

ica 偏向概率方向
	实现复杂，虽然从理论上讲更精确，但是有时候独立成分不存在
pca　偏向线性代数方向
	实现成本低，不容易掉进局部极值，清晰，但是结果可能并非所要（完全线性）
		主成分始终存在
	

http://blog.csdn.net/watkinsong/article/details/38536463

		对于一个训练集，100个对象模板，特征是10维，那么它可以建立一个100*10的矩阵，作为样本。求这个样本的协方差矩阵
		，得到一个10*10的协方差矩阵，然后求出这个协方差矩阵的特征值和特征向量，应该有10个特征值和特征向量，我们根据
		特征值的大小，取前四个特征值所对应的特征向量，构成一个10*4的矩阵，这个矩阵就是我们要求的特征矩阵，100*10的样
		本矩阵乘以这个10*4的特征矩阵，就得到了一个100*4的新的降维之后的样本矩阵，每个特征的维数下降了
			而自己乘自己得到的正好是各个样本这一维度上的方差，一举两得
