感知器
	就是根据输入产生权值，然后叠加后，根据阈值获得离散输出
	因为是线性的，所以两个变量的话，就是一个半平面
	可以用这个制造and or，只要w 和阈值给丁特定的值就可以
		xor = or - and需要两层感知机

感知器规则：
	对于感知器的w权重参数：
	可以通过 k*(y-y_real)*x(i)  来进行调整，
		这里k是学习率，可以理解为调整的step跨度，
		y-y_real这个可以得到结果是否需要变化，以及变化的方向，
		x(i)代表当前变量对结果是否有影响（0，1）  
		通过小夸度不断微调，所有参数都会倒位，只要
		保证存在线性可分
	缺点是如果不能保证线性可分，那么可能会无限循环
	n(y - y_real)x(i)  y_real是激活函数的结果阈值化之后的结果
		 这个就是w(i)的变化学习率

	另一种感知机规则：
		ｗ　ｂ开始之后，每次都是利用k*(y_expected)*x(i)  加到==> w
									k*(y_expected)  ==>b  
			因为感知机输出只有两个值，－１，１
			可以用向量的思想http://www.jianshu.com/p/3d316f380041可以看出，就是向量根据
				差不断纠正，这里ｙ起到纠正方向的作用，和上面的规则实际效果是一样的，
				而另一种形式的对偶展开，其实就是这里的ｗ利用这些累加式子代替，带入原式的结果

梯度下降规则：
	就是对结果值不做阈值处理，然后直接和目标结果相减，
	取所有输入的差平方和，然后往误差最小的方向走
	最后结果和感知器比较像
	n(y - a)x(i)  a是激活函数的结果 这个就是w(i)的变化学习率
	使用激活函数而不是阈值化后的值，是因为阈值化之后不可导
	相见　梯度下降的笔记：
		其实批量就是在不更新的前提下求出全局最优方向，
		随机是在当前的前提下选择一个方向下降，所谓方向，就是各个向量的比例

比较：
	感知器规则这个如果线性可分，可以得到最优解，但是不可分的就没辙了
	梯度下降可以得到局部最优解

sigmoid:
	s形函数， 1/(1 + e^(-a)) 这个是关于激活函数a的函数，
		如果a-》正无穷，值1， 负无穷，值0，这样
			主要变化在-5到5之间
	可以利用这个，就可以应用梯度下降在激活函数上了，
		就可以结合前面两者的优点

反向传播：以对计算有利的方式来组织计算链
		误差通过结果的差来反馈，然后调整输入

避免复杂化：神经网络
	如果层数过多，节点过多，权重过大，都会导致复杂化
	中间层都叫做hidden layer
	
	同样使用交叉验证 来防止过度拟化

限定偏差：
	我们使用的东西，权值函数，激活函数，避免复杂化，sigmoid

优先偏差
	权值从小的随机数开始，小是因为大的化本身复杂化
		随即是不会每次都在同一个地方结束


感知机支持在线学习：
	因为一次处理一个输入

激活函数：
	如果没有这个，那整个网络输出都是线性函数，无法表示非线性

具体的感知器训练过程 ：
	首先定义start  感知器的权重，一般是小的随机数
	然后输入input，包括输出label
	用期望输出减去实际输出，然后乘以学习率，乘以对应的输入项，
		还要乘以激活函数的导数，这个在wiki的delta rule 里面
		这些是根据最小平方差对每个权重求偏导得到的，
		这代表了梯度下降最快的地方，所以往这个数值走
		其中如果有阈值作为第一项，那个系数为1，（取的权重
		是阈值本身）
	对于每个样本，对于其中的每个权重，都这样干，这整个算一轮
		一轮要做完，这叫做批量梯度下降

	权重的调整就是原权重的取值加上这里的delta值
		其中激活函数的导数这个用于连续函数的调整，
		这个是对线性求和之后的数求导数！！，最后乘以的
		是这次用来测量的输入向量，由此得到对应的权重
