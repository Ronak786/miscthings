感知器
	就是根据输入产生权值，然后叠加后，根据阈值获得离散输出
	因为是线性的，所以两个变量的话，就是一个半平面
	可以用这个制造and or，只要w 和阈值给丁特定的值就可以
		xor = or - and需要两层感知机

感知器规则：
	对于感知器的w权重参数：
	可以通过 k*(y-y_real)*x(i)  来进行调整，
		这里k是学习率，可以理解为调整的step跨度，
		y-y_real这个可以得到结果是否需要变化，以及变化的方向，
		x(i)代表当前变量对结果是否有影响（0，1）  
		通过小夸度不断微调，所有参数都会倒位，只要
		保证存在线性可分
	缺点是如果不能保证线性可分，那么可能会无限循环
	n(y - y_real)x(i)  y_real是激活函数的结果阈值化之后的结果
		 这个就是w(i)的变化学习率

梯度下降规则：
	就是对结果值不做阈值处理，然后直接和目标结果相减，
	取所有输入的差平方和，然后往误差最小的方向走
	最后结果和感知器比较像
	n(y - a)x(i)  a是激活函数的结果 这个就是w(i)的变化学习率
	使用激活函数而不是阈值化后的值，是因为阈值化之后不可导

比较：
	感知器规则这个如果线性可分，可以得到最优解，但是不可分的就没辙了
	梯度下降可以得到局部最优解

sigmoid:
	s形函数， 1/(1 + e^(-a)) 这个是关于激活函数a的函数，
		如果a-》正无穷，值1， 负无穷，值0，这样
			主要变化在-5到5之间
	可以利用这个，就可以应用梯度下降在激活函数上了，
		就可以结合前面两者的优点

反向传播：以对计算有利的方式来组织计算链
		误差通过结果的差来反馈，然后调整输入

避免复杂化：神经网络
	如果层数过多，节点过多，权重过大，都会导致复杂化
	中间层都叫做hidden layer
	
	同样使用交叉验证 来防止过度拟化

限定偏差：
	我们使用的东西，权值函数，激活函数，避免复杂化，sigmoid

优先偏差
	权值从小的随机数开始，小是因为大的化本身复杂化
		随即是不会每次都在同一个地方结束


感知机支持在线学习：
	因为一次处理一个输入

激活函数：
	如果没有这个，那整个网络输出都是线性函数，无法表示非线性

具体的感知器训练过程 ：
	首先定义start  感知器的权重，一般是小的随机数
	然后输入input，包括输出label
	用期望输出减去实际输出，然后乘以学习率，乘以对应的输入项，
		还要乘以激活函数的导数，这个在wiki的delta rule 里面
		这些是根据最小平方差对每个权重求偏导得到的，
		这代表了梯度下降最快的地方，所以往这个数值走
		其中如果有阈值作为第一项，那个系数为1，（取的权重
		是阈值本身）
	对于每个样本，对于其中的每个权重，都这样干，这整个算一轮
		一轮要做完，这叫做批量梯度下降

	权重的调整就是原权重的取值加上这里的delta值
		其中激活函数的导数这个用于连续函数的调整，
		这个是对线性求和之后的数求导数！！，最后乘以的
		是这次用来测量的输入向量，由此得到对应的权重
