集成学习：
	利用生成的规则进行判断
	多个学习集合，组合规则

	bootstraping:
		每次可放回的选择一个数量的样本，应用统计，得到结果
		多次平均
	jackknife:
		每次去除几个样本，多次平均
	这两个都是以下集成学习算法所代表的思想

bagging:(bootstrap aggregation)
	可放回的选择子集抽样（所以即使选择数量和总数一样，也有可能选到不少重复的）
		然后对每个子集应用自己的分类方法，然后平均即可
		也可以投票

random_forest:
	和ｂａｇｇｉｎｇ一样抽样,不过抽样次数和总数一样多，
		子方法选择决策树,但feature也是抽样的，然后决策树

extra_trees:
	每次随机森林选择的时候，直接随机选择一个ｆｅａｔｕｒｅ

boosting:  意思是利用现有资源自己搞定
	开始时候所有元素权重一样，每轮分错的增加权重，
		下一轮在此轮基础上分，每轮得到的结果也评估一个权重，
		所有轮的结果综合平均或投票
	adaboost 有改进
	
	这个比ｂａｇｇｉｎｇ要精度高，但是不能并行，神经网络这种会很慢
		而且少数情况会退化

adaboost:
	这个就是利用每一次计算当前的损失，每次选择损失最小的那个分类器，而这个损失的定义，
		就和我们设定的权重有关（所以这个权重不是用在分类器里面的，而是在计算损失函数的时候用的，
		）同时还有计算每次分类器拥有的最终平均时候的权重的时候有用，
		ｎｏｔｅ：每次计算当前一轮的时候，都是在上一轮全部确定的前提下的，损失函数的定义也会用到
		到目前为止已经做出来的分类综合函数，每一步都是这样子迭代上去的
	每一步的预测函数的权重在ｗｉｋｉ上有，可以用权重的公式得到
		权重就是上一步的函数带来的预测与真实的损失作为乘数因子和上一部的权重得到的总数
		而当前为止的综合函数，就是以函数权重（利用损失权重得到）为因子的乘积和以前的函数的和
	这个的特点是每一步计算最小的当前损失，来选择当前一步的分类器

gradient boosting:
	这个每一步计算的离目标的差的最大下降方向，然后选择当前分类器来满足最大的弥补作用，
		所以这里的分类器的作用是做到最大的弥补当前和真实的差，最大：用梯度下降来衡量
	具体过程：参照wiki
		首先选择一个可以使得ｌｏｓｓ最小的base learner作为开端
		然后每次都计算到上一轮为止的那个大函数的损失函数的梯度，这里的开梯度的参数
			是之前的前一个总函数，因为我们要计算的就是这个函数的下降方向
			得到梯度后，从当前的base_learner里面以梯度为ｌａｂｅｌ，ｘ为向量输入
			再找最小的损失函数的那个base_learner(因为我们这里就是要最小化损失，就是
				要尽力满足梯度为标签的输入,本来这一步不用的，新加入的就应该是梯度，这样简单，
				但是实际我们这里不能随便加，要加分类函数，我们需要分类,而且是希望对预测的
				也能有好结果，所以不能直接加梯度数值，但是完全符合的函数又难找（不能空手变出来吧？）
				所以找最相近的）
			找到后，这个就是你要的base_learner替换后的梯度，然后计算一个步距因子，
			是的相乘加上上一步总函数后损失最小，那这一步的就是这个总函数了



误差：
	这里指按照权重分配的误差，而不是平均概率
弱学习：
	就是对于任意的随机分布，都可以做到比随机的概率要好，
	就是对任意分布，这个学习方法都可以学习到东西

	存在若学习分类，就是说对于当前的额数据分布，存在可选的一种
		假设，可以让分布的正确率高于i随机情况
	不存在，就是在可选的假设集合中，一个都做不到对当前的数据分类

	这个是个分布的学习过程，会在不断的误差中调整每一项所占的比重

	这个实质上就是一个分配比重的理论，学习的过程还是其他的算法，
		只不过里面的每个元素要按照现在的比重分配
		在使用多次根据不同比重的切分之后把所有的组合起来，
			会发现最后得到的分割线是很好的

函数里有个置信度的概念，就是答案正确，但是里分解点太近

`	弱学习趋向于把数据很好的分成两类，而不会产生过拟合，
		如果有过拟合，那就是加权算法本身的问题

		找到多个随机分类数据集后，把多次的分类数据集各自
			进行弱学习调参数，最后平均求和
过拟合：
	神经网络
	粉红噪声：均匀（高斯噪声是不均匀的，白噪声）
