集成学习：
	利用生成的规则进行判断
	多个学习集合，组合规则

bagging:
	随机选择子集，然后找到拟合函数：
	最后找到所有子集函数的平均函数？
	一般随机找子集就是均匀随机即可

boosting:
	不是随机选择子集，而是选择最难的子集来创建规则，
	也就是当前分类最不准确的子集

误差：
	这里指按照权重分配的误差，而不是平均概率
弱学习：
	就是对于任意的随机分布，都可以做到比随机的概率要好，
	就是对任意分布，这个学习方法都可以学习到东西

	存在若学习分类，就是说对于当前的额数据分布，存在可选的一种
		假设，可以让分布的正确率高于i随机情况
	不存在，就是在可选的假设集合中，一个都做不到对当前的数据分类

	这个是个分布的学习过程，会在不断的误差中调整每一项所占的比重

	这个实质上就是一个分配比重的理论，学习的过程还是其他的算法，
		只不过里面的每个元素要按照现在的比重分配
		在使用多次根据不同比重的切分之后把所有的组合起来，
			会发现最后得到的分割线是很好的

函数里有个置信度的概念，就是答案正确，但是里分解点太近

`	弱学习趋向于把数据很好的分成两类，而不会产生过拟合，
		如果有过拟合，那就是加权算法本身的问题

		找到多个随机分类数据集后，把多次的分类数据集各自
			进行弱学习调参数，最后平均求和
过拟合：
	神经网络
	粉红噪声：均匀（高斯噪声是不均匀的，白噪声）
