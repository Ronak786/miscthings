plot:
	划线：
	matplotlib.pyplot
		scatter 画点
		plot  画线
		xlabel  ylabel
		show  to show
LinearRegression:
	sklearn.linear_model.LinearRegression
	fit(train, result)
	predict(xxx)  这个只能接收list!!

估计：
	score:  r^2 score
		输出由输入所决定的百分比，相性
	可以比较训练和测试集合的分数差
	使用平方和来衡量很不标准，因为数据两大，就会大

Note:
	注意数据的格式，就在文档的shape里面，注意要纵向量的，就是两层数组
	slope 返回 二维数组
	intercept返回一维数组
	predict return 二维数组 , 接受二维数组

最小化误差的方法：
	最小二乘法 这个利用偏导为０，直接的出矩阵形式的结论，计算比较复杂，有时候没哟解释
	梯度下降法 这个逐渐逼近，优势后会有局部最小解
	
	使用平方而不是绝对值，这个对大的差惩罚大，对于支持向量机
		更加有效记录，而且微分性质好

逻辑会对更偏向两边的数给予更加偏向０　或者　１的值，
	让他们的影响更加小，，所有和单单线性回归＞０　＜０是不太等价的

	逻辑回归　虽然本质上就是线性的方程然后加上一个sigmoid函数，但是这个在优化
			过程中不会单单往线性的方向靠，而是逻辑函数导向的，所有最后的ｗ参数结果其实是和
			线性回归中不一样的，　　这个更加注重分类，以及分类的鲁棒性，不会受额外点的严重影响

			同时，损失函数不能用线性的那一个，因为那个存在局部优化点，需要用另一个
			  -log(h(z))  if y == 1   a
			  -log(1-h(z)) if y == 0  b
			  使用的时候　　　y*a + (1-y)*b

	逻辑回归的误差，
		要用极大似然估计来推算

残差平方和 这个就是表示偏差程度的综合，用来梯度下降中找梯度的
均方误差：　这个就是平方和除以ｎ用来表示平均变化程度的

局部线性回归：
	就是利用knn的思想，局部数据做回归预测线
