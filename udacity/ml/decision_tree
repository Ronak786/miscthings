decision tree classifier
	这个用于离散变量，出来的是原来结果中的离散值
decision tree regression
	这个用于回归，也就是连续

sklearn.tree.DecisionTreeClassifier的
predict

parameters:
	min_sample_split: 最少样本分割数，就是只有当前node剩下的样本数量
		至少是当前水准的前提下，才能继续分割

entropy:
熵的实质：
	就是最优化编码的平均长度，如果只有一种，那平局长度就是１，纯的，
		对象越杂乱，长度越长
		这里已经是变长编码了

	熵，衡量决策树中的不纯度
	决策树分割就是要尽量找到使得某一个部分熵最小的分割点
	公式 entropy = sum(-p(i)log(p(i)))
		这个是对于你选定的一种分割计算的，基于那种分割，
		左右两边所占的rate就是p(i)

信息增益：
	父节点的熵减去子节点的加权熵综合

拆分规则：
	父节点中根据标签 计算出熵
	子节点各自根据当前分割计算出熵，然后根据当前节点占
	父节点总数的百分比，计算加权和，减去父节点，得到信息增益

方差-偏差困境
	偏差高就是忽略了一些数据，导致无法描述
	方差高就是太关注具体数据，导致无法泛化

缺点：
	很容易过度拟合，要控制分割书数量
