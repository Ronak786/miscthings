decision tree classifier
	这个用于离散变量，出来的是原来结果中的离散值
decision tree regression
	这个用于回归，也就是连续

sklearn.tree.DecisionTreeClassifier的
predict

parameters:
	min_sample_split: 最少样本分割数，就是只有当前node剩下的样本数量
		至少是当前水准的前提下，才能继续分割

entropy:
熵的实质：
	就是最优化编码的平均长度，如果只有一种，那平局长度就是１，纯的，
		对象越杂乱，长度越长
		这里已经是变长编码了

	熵，衡量决策树中的不纯度
	决策树分割就是要尽量找到使得某一个部分熵最小的分割点
	公式 entropy = sum(-p(i)log(p(i)))
		这个是对于你选定的一种分割计算的，基于那种分割，
		左右两边所占的rate就是p(i)

	
熵的推论方法：
	当时应该是这样的，
	信息的表示需要使用一个数学公式，必须满足信息的可加性，同时概率的独立可乘性
		在数学上满足的式子只有　-log(p(i))，而且验证满足
		并且后来证明这是平均最短编码方式（如果使用２为底，满足计算机条件）
	交叉熵：
		这个就是用真是的概率去乘以误用的编码方式，得到的结果，
			可以证明一定比不误用的时候高，所有可以利用这个当作损失函数，
			这个比均方差的优点在于，这个是误差导向的，而均方差在有ｓｉｇｍｏｉｄ的情况下，
			导数容易趋于０，往后梯度下降会变慢，这里不会，所有这个计算有优势！！
			香农厉害！！

信息增益：
	父节点的熵减去子节点的加权熵综合

拆分规则：
	父节点中根据标签 计算出熵
	子节点各自根据当前分割计算出熵，然后根据当前节点占
	父节点总数的百分比，计算加权和，减去父节点，得到信息增益

方差-偏差困境
	偏差高就是忽略了一些数据，导致无法描述
	方差高就是太关注具体数据，导致无法泛化

缺点：
	很容易过度拟合，要控制分割书数量
