SYNCHRONIZATION ISSUES
	lock   scalable clean , must be thought when start design , not after work!!

	ensure unsafe concurrency  and race not happen	, so make critical regions a atomic instruction.
		so we use lock , we have diff types of locks , diff in behavior when lock can not be held
		and the lock code itself is atomic assured	
	user space : pseudo concurrency , multi-pro  , true concurrency
	kernel space: interrupt  softirq tasklet workqueue ,   preemption ,  smp (sleep in cirtical is not allowed)
	
	lock should design beginning ,not afterthought
	we have  smp-safe preempt-safe  interrupt-safe

	the smp and preemption will add lock code to kernel , if not CONFIG_SMP  CONFIG_PREEMPT , those will 
		disappear ,  so choose when needed

	MUCH TAKE CARE OF  global data and share data between interrupt, smp , thread , process , func

	
	DEAD LOCK
		do not lock twice , 
		lock in order
		lock as simple as possible
		make sure procession will finish,so lock will be released absolutely

	lock is needed by more and more scalibility system
	locks begin with a big kernel lock , evolving into nowadays fine-grained lock ,
		but should not be more fine-grained , will add cost to lock overhead
	start simple and grow in complexity only as needed , simplicity is key
	
ATOMIC
	we have atomic integer  and atomic bitwise  implementation , 
	test_bit is atomic implementation , __test_bit() is non atomic

SPIN LOCK
	spin lock will busy waiting , always should disable interrupt , spin_lock_irqsave/irqrestore , 
		(note this lock will disable preempt)
		spin lock can disable interrupt  , bottom half (have some type of funcs)
 
	rw_lock can be acquired(spin lock implement),  
		read can be just acquired in interrupt , but write must disable interrupt
		kernel prefer  read  than write!!
	
		note:  also have rw_semaphore  ,this diffs that can sleep
			and both use uninterruptible  versions' read & write,
			have a downgrade_write to down write into read

	semaphore  can sleep , but have much more overhead ,so short  duration should use spin lock
		 only used in processor context (sleep ) ,can not acquire when having spin lock,
		 , also not  disable preemtption

		we use  down_interruptible  down()  up()  down_trylock sema_init(), to control
			the interrupt means when wait for lock to acquire , can accept signal
			and return fail to get
NOTE:   DECLARE_MUTEX   init_MUTEX used in semapthore
	DEFINE_MUTEX    mutex_init  used in mutex  !!!
	ofter mutex  better than semaphore

	mutex   
		mutex defined for effecient , 

		mutex must be get and release by the same processor , 
			can only be used in processor context ,
			more stricter rules  in Page 196

		
	compeletion :
		DECLARE_COMPLETION()   init_completion  wait_for_completion  complele() 
		when called complete() ,any processor waited on the complete var will be signaled up	

	BKL  big kernel lock
		used only in process  context , 
		can be auto released while sleep ,
		will disable preempt
		use lock_kernel()  unlock_kernel()  kernel_locked()
		
		used when early in kernel 2.0to help smp , now should not used in new code
			mostly used in sync func calls

	seq lock:
		this can be used in interrupt context,
		write_seqlock() write_sequnlock() ,  
		but read in this form
		do {
			read_seqbegin(&xxx)  // the lock
			xxxxx //do some thing simple
			
		} while (read_seqretry(&xxx))  //means the number read now diffs from the number in read_seqbegin
			// so this time's read mixed with write , so must loop reading again
	
		this is lightweight , when have a lot readers , few writers ,data simple , 
			writer never afftec readers' numbers 


	kernel preempt:
		can be disabled enabled by preempt_disable  preempt_enable , also nestable , preempt_enable_nosched()

	barriers: (compiler knows current context's reorder or not , but interrupt happens or not ,don't know)
		mb()  rmb()(read)  wmb()(write) , memory barrier used to set barrier meaning that (load/store/all)
			operations before the barrier will be done before those instructions after the barrier
		smp_rmb  smp_mb smp_wmb  smp_read_barrier_depends() , barrier()(prevent compiler from reordering)
			 these will be memory barriers in smp , and compile barriers in up(uni processor)
		read_barriers_depend() ( this is faster)

		compiler , processor will  reorder two instructions , so in kernel , barrier is important
			eg.  a=1 ; a = 3 ; b =a , now b == 1 or b==3 ? 



TIME MANAGEME
	the time between two interrupt handlers is the tick , which is 1/(tick_rate) , kernel use this to measure time	
	when interrupt issues , update system time , boot time , dynamic timers' count ,balance scheduler's queue , update
	processor statistic and  resources

	kernel notion of time is  from the  system timer ,which update periodically by the interrupt , with frequency of
		HZ ,which is defined by CONFIG_HZ(usually 100 ~ 1000)
	with resolution grow up , accuracy  grow high
	
	the poll select() will waste less resource waiting
	the scheduler will be more balanced ,(if hz is small , tick time is high , then schedule lantency is big)

	TICKLESS 
		a tick one will interrupt every tick, but tickless one will only issue interrup when need some work to do,
			when totally idle , will not tick any more!! saving power and system overhead,
			but will add some expense between userspace and kernel space

	 	a jiffy is a tick , between two interrupts' time  
		 	it is type unsigned long ,and is volatile , so will not be optimized

		jiffies define  32bit and 64bit diff , but occupy same memory , 32bit is low bit of 64bit,
			in 32bit, we have special func to access 64bit jiffies64 , but 32bit is enough ,
			in 64bit , we will always access  64bit jiffies== jiffies64


	system have a rtc ( maintain the wall time, mostly for user to know time of world)
		also have some system timerses ,the main is system timers
		every tick ,the system timer will trigger the time interrupt  handler , 

		detail : interrupt handler
			update jiffies , update wall time , update process time statistics ,
			update schedule issues , monitor whether local timers have expired ,
			set the softirq to do local timer funcs in bottom halves

		we can use init_timer , add_timer , mod_timer to init , add or modify the added timer ,del_timer_sync to 
			delete timer and wait for the already running handler to stop

	we also have no system timer  delays. (just processor block for a little time)
	1 just busy looping the specified jiffies (silly!!)
		we also can use cond_resched() to instead of busywaiting ,but never use some func that can block or sleep
	

seconds :  1 second = 1000 milisecond(ms) = 1 000 000 nanosecond (ns)= 1 000 000 000 microsecond(us)

	when we really need smaller than 1 tick's delay , we can use  udelay(micor)  ndelay(nano)  mdelay(mili) ,
		they busy waiting , so should use with very small duration

	about bogomips , bogomips be calculated when boot , and be used by udelay to calculate how many iterations 
		should be used

	use schedule_timeout() to sleep a specific time and then waked up by kernel , (so should in process context 
		and not hold a lock) ( the schedule_timeout will put process sleep for sometime and wake up it / or waked
			up by interrupt)


MEMORY MANAGEMENT
	32bit and 4kb pages , 64bit has 8kb pages

	struct page :  used to describe physical page(every has one!!),not virtual pages , so according with swaps and mappings , 
		they will associate with diff virtual pages ,
		member:  virtual --  virtual address
			 count  --  page refernce count

	kernel divide memory into some zones , depend on diff archs , we have diff zones containing diff memory areas
		ZONE_DMA  ZONE_NORMAL ZONE_HIGHMEM
 	we use struct zone to represent zones
	
	the HIGH MEMORY problem:
		in old days , when 32bit , when you use more than 4gb memory, kernel can not map them all in 
			virtual memory , so high memory(a portion in kernel space) be used to dynamically 
			map them when needed 
			but in 64bit ,available vm is too much , so high mem no more needed

	when allocate , we use alloc_pages(return page struct) as a base low func , 
		above , we have alloc_page , __get_free_pages(return logical addr) , __get_free_page , 
			get_zero_page ,kmalloc(alloc physical&virtual continous addr) , vmalloc(alloc virtual continous addr,
				but more performance cost , so used only needed) , kfree , vfree
		on all these funcs , we have flags ,gft_t ,  used to specify which context and mem zone the allocate func use,
			often used ones are GFP_KERNEL(used in process context and can block) ,GFP_ATOMIC(used in interrupt or
				spin lock context , not sleep)
			remember vfree  kfree can both sleep!!
		
	
	
SLAB LAYER:
	in kernel , we use slab to manage the allocation of memory , by groups of struct type , so as to make alloc and memory management 
		more efficient and less fragments.
	use struct kmem_cache  to represent a cache( a type of data that will be allocated) , in which we have lots of slabs ,which have 
		lots of same data structs with physical addr continously.
		
		details when alloca: ,first , used kmem_cache_create to create a cache ,
				then , kmem_cache_alloc , kmem_cache_free to get the space to store structs and free
				then kmem_cache_destroy at last to free the cache whole.
	
	about store vars on kernel stack:
		the stack is small , often one or two page per process .when one , the interrupt will get a unique page per processor.
			use alloca instead of stack to allocate big data structs.

	when use high memory with alloca_pages() , we have no virtual addr , so we use kmap , kunmap to map . (will sleep) , or
		kmap_atomic() ,(will not sleep , so better in interrupt handler)
	

PER CPU VARS
	we can set an array , then use get_cpu() (will disable preempt) , put_cpu() (enable preempt) to get the cpu id , the
	 	store and load the value in the array
	also we have DECLARE_PER_CPU()  DEFINE_PER_CPU , get_cpu_var() set_cpu_var , per_cpu() ,to get the value , 
		the setXXX used to enable preempt , not do anything else

	but at runtime , the modules's per cpu vars must be allocated by  alloc_percpu , then free_percpu
	
	these per cpu vars can reduce locks and cache flushed and improve performance

CHOOSE:
	want contiguous physical addrs , use kmalloc or low level interfaces like alloc_pages , 
	want in interrupt context , use gpf flag  GFP_ATOMIC instead of GFP_KERNEL
	only want virtual , use vmalloc
	want high memory , use alloc_pages , then kmap , 
	want per cpu vars , use those interfaces	


VFS:
	the virtual filesystem used to provide generic interface 
	all filesystem have  struct super block ,  inode , file , dentry

	struct super_block  do with inode infos and the fs infos and all actions to do with inode
	struct inode used to do with a signle inode (specific type) ,and those funcs will  do with a single inode and permissions
	struct dentry used to represent the path a file lookup will experience , so all files and dirs on the path will be represented by
		a dnetry ,  usually pre allocated in a slab , 
		the dentry has three kinds , used , unused , negative , 
		used: in point to a valid inode and in use
		unused , point to a valid inode but not in use,cached in case of future use
		negative , no assocaited with a valid inode , but still can be in use , because program will seek an nonexist file,which will
			also take a long way to look for then return enoent ,this cache will speed up this return
		dentry  details:
			all dentries in a hash table , when resolve a path  first look up in hash table by d_lookup, if no , then resolve from
			head , then add to cache , because files often be accessed localy .so cache will speeed up the lookup

		this struct is all about path (also file name )

	struct file , related to a process opens that file , this have a struct path -> struct dentry -> struct inode to a real file
		this struct has operations related to what we will use to a file(oepn close sync flush ...)
	

	then file_systemc_type struct  represent a kind of struct ( a kind only one) , this used to get put super block and store 
		list of file system and list of superblocks of that file systemc type
	
	when mount , we have a vfsmount struct to represetn that mount point

	
	from the view of process ,in the process descriptor , we have struct file_struct  ,struct fs_struct , struct mnt_namespace , 
		the former two related to the struct file  and   dentry&superblock that the process related to , the latter one represent
		the unique view of the file system view , which means each process has its own  filesystemc view ,two process has diff namespace
		will result in two same pids , but each process will only see the one itself owns

		

BLOCK IO LAYER
	char devices read sequencially , so almost not need a subsystemd , just read . but block device read randomly as blocks , so optimise important ,
	a sector is how small a hard device can do with , commonly 512byte .   
	a block iis how small a software can handle, usually multiple of a sector and not bigger than a pagesize , so 512byte , 1k , 4k is common
	in memory , a buffer represent a block , and is described by a buffer header struct ,   
		this used to describe the relation between in memory buffer and the corresponding devide sectors on hard disk, but not joined in transmission
		kernel will directly manage on pages

	but now we use struct bio in place of struct buffer_head
		bio represent an operation of io , have lots of discontigous pages , each page represent by struct b_io_vec , have bi_vcnt structs , and the current
			is being handled is the bi_idex th , this is more lightweight , because you only have to maintain one struct for all io operations instead
			when using buffer_head ,you must split io into lots of structs and maintain their relations , it also maintain a count of reference , when
			not referred , destroy
		now buffer head still in use to represent  buffer .

	request queue( diff from work queue ,which used in bottom buffer , when queue in , will do whenever sheduled to and have not empty in queue)
		a request have multiple bios , as long as they are consecutive blocks on block

	NOTE : a strut bio represent an io , this is contigous on disk ,but not in memory , so they can be scheduled (although not contigous in memory),
		then a request queue  can have multiple bios , as long as they are contigous in disk(although not in memory contigous)

	IO SCHEDULE
		used to merge and sort requests that kernel receive so that requests submitted to the hardware will be in an optimized sequence ,and reduce seek time.
		merge when  two requests have sectors adjacent , sort in order to seek in a direction
		back merge means new one just add to the tail of the old one , commonly seen than front merge

		the linus elevator most simple , just seek is can merge , then if existing too old (then just  tail the new one at the end) , then if can insert ,
			then just add to tail.(four tryings) ( but this will still starve some old requests)

		deadline io:
			read lantency is important than write lantency , because , when read , apps often have to wait until data com . but when write , just wirte,
				it is async , not have to assure they have be written into the disk block
			deadline try to minize starve while keep throughput
			
			details:(the linus with a time fifo)
				when a request come , add into the sorted queue like the linus , then again add into a read queue or wirte qeueu(just a fifo), 
				because the fifo maintain a time expire , when the head of the fifo expires(must be the head , because fifo) , then do
				that request instead of the sorted queue(add to dispatch queue , that will be read by the driver) , 
				and read has less time than write .so perferred to read
			
		anticipate io:
			like deadline , but every time perform a read , will stuck some time(some millisecond) wait if some other read coms , it so ,and adjacnet, then
			do that,   so this is better for write heavy,but some frequently read operations .can save the seek back and forth time. 
			it also record per process infos to optimise this method

		cfq io:
			maintain a queue for each process , sorted and merged , and round robin , every queue get four to process before go to another.
			works quite fine in most environment , so is default in linux
		noop io:
			just merge , not any thing else , intended for flash memory cards ( which have no overhead for seek) 


		when startup , use  elevator=as/noop/cfq/noop to override the default 
		

PROCESS ADDRESS SPACE:
		every process has a virtual address space , in which ,has many mamory areas with respective permissions
		all memory a process need in it ,including data ,text , bss ,stack , share .so , mapped file , share mem , malloc , represented by a struct mm_struct in process descriptor
		in struct mm_struct , we have mm_count and mm_users , they are diff , the former count all processes using that mm_struct as one , while the latter count as the real count.
		but the latter never count other than process , so if nine pros , then  mm_users = 9 ,and mm_count =1 , if another kernel thread count that , then mm_count be 2 (more detail 
			see ULK).
	
		threaded tree: use a rb tree to store memory areas , then overlay a link list on that tree to optimize traverse ,the process address space use this two methods to access specific
			memory areas
		all mm_structs linked in mmlist , and the init is init_mm (init's mm_struct)

		if CLONE_VM flag set , then when fork , mm_struct will not created , instead , a point from child will to father and mm_users will add by 1

		when exit , exit_mm will call mmput() to dec the mm_user , which if 0 , ->mmdrop() to dec mm_count ,if 0 ,->call free_mm() to return to mm_cachep in the slab

		kernel threads not use user space address , but do need kernel memory map infos , so the address space of the previous scheduled process will pertain for the
			kthreads to use(just kernel portions)
		VMAs vritual mamory areas , the struct describe one of memory areas of mm_struct include , 
			the memory has its own flags , diff from struct page flags , this describe something kernel can do to that area , like read ,write exec , share ....
				the flags can even optimise read ahead or disable by a flag
				each mem areas also as its own operations vm_ops of the struct vm_area_struct)
		the mm_struct (has vmarea and rb_root)->  vm area -> next /  rb_node

UTILITY: 
		pmap used to see a process's mapped address , like /proc/pid/maps

		in memory nonwritable or shared areas only keep one copy in mem , so the exact mem occupy is small
			all zero pages originally mapped to one place , the zero page , but once written into , copy on write .

		some help mapping funcs:
			find_vma() find_vma_prev()  find_vma_intersection()  ,used to find the vm_area_struct containing the specific address or just bigger than that address ,
				the prev also return the area before that addr ,the intersection return vma just inside the given start and end addr
		
			
			do_mmap is the func alloc an area of mem for some use , will create a new struct area or merge into an exist one 
				in user space , implemented via system call mmap2 , which support user space lib call mmap()
			do_unmap  and unmap() to unmap

		page table : (translate job mostly done by hardware , kernel just set those pages)
			used to convert virtual address to physical addr  by splitting into portions and each portion as an offset in a table
			,and also have some cache in the TLB table(directly translate into physical pages) , 

PAGE CACHE:(kernel maintain  some page based cache for all files , so common writes just write to page, the cache to disk is maintained by kernel , read also , just read from cache)
		often accessed data on disk should be cached , for subsequent access , cache stored in physical pages
		when write to cache , we have directly write into disk ,fail the cache /   write both to cache and disk /  write to cache ,use process daemon to write into disk(linux use this)

		when mem is needed , we use cache  evict , ( somtime write back before that)
			evict stratigy:
				least recently used :  keep a timestamp or a list sort by time , then remove the LRU . 
				two lists (linux use) : keep two lists , active inactive , only the one in inactive can become active , so if just used once , in inactive . and evict will
					be done through  inactive

		linux use a struct address_space   to represent a cache of a file , a file only have one thus struct
		 we have a strcut of operations to operate cache (specific to what filesystem it belongs to)
		use find_get_page()(use a radix tree to speed up) to get from cache , if not exist ,allocate one cache and add into cache.(read)
		write:  get cache(ifnot , allocate) , prepare_write ,copy from user , copy to cache , commit to disk

		we have buffer cache and page cache , now buffer cache same as page cache will simply point to that , but for no device based files , just buffer cache
			one buffer represent to one block on disk

		write back maintains by kernel , one is a timer waked func periodically to flush old enough caches,
				another is invoked by kernel when not enough free mem , so need flush and then free cache(use multiple threads)
					(about the flusher threads , at first ,only one thread; then a bunch of dynamically allocated threads ,but can congested on
					the same disk , so last , use threads specific to the disk)
				thrid is a userspace sync
		
		laptop mode , this will flush the old pages including all dirty pages , to save hard disk up time and up powers for laptop , but will lose lots of data when crash



