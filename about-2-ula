memory management
	in ia32 , memory directly mapped into kernel only less than 1gb , so others are controlled by
		high memory , and mem  above 4gb can only be controlled by cpu's PAE extension , 
		and everytime can only control 4gb ,
	
	high memory :
		this problem only happens on 32bit cpus , and only when kernel want to access
			memory above 896mb, because kernel always map physical page directly 
			into kernel space and not use the page table model the user space used .
			so this is not problem  for userspace programs , they always use page
			tables ,and that pagetable always have values valid
	numa:
		distribute computer, multi-cpus have their own mem , but access other cpus mem through
			qpi channel , will be a little slower.  when core are too many , access to mem
			from bus can be a bottleneck , so numa maybe better

	memory struct: generically numa , we have node  ,in which we have mem zones , every zone have specific usages ,likke ZONE_DMA ZONE_HIGHMEM ZONE_NORMAL
			 in uma , we only have one zone

			we use a struct pglist_data(pg_data_t) to represent a node ,will include all zones and pages that node have (we also
				have a enum flag to represent to status of node ,especially N_HIGH_MEMORY(high priority)  N_NORMAL_MEMORY
					(only we have multi nodes)
			we use a struct zone to reperesent a zone , and the add padding used to imporve the cache effection
			detail struct zone:
				first part: page's allocate and recycle decition and mem management
					    reserved mems , page in cache or not , idle pages's number , brother system
					   this area used to judge the current state of the memroy system
				second part: 
					record page's activity and all pages's state(can be recycled or not)
					processes's wait table , and all pages numbers start place

				page_min page_high page_low:
					system will set page_min page_high and page_low  considering the reserved pages (which
						is set according to how many mem in the system) most times the page_min is the min_free_kbypes
				pageset:
					use hot and cold to represent the pages are in cpu cache or not(every cpu has one member in array for every zone)
					the batch defines a cache can contain pages number
				struct page:
					store info about referenct of page ,the address of page ,and if highmem exist--- the high mem addr is mapped to kernel.
					the page flags implies page's status locked(not accessed by others) ,updated ,lru , highmem , private , active , swap ,compound and so on
					,all arch independent
					we also have wait_on_page_writeback  wait_on_page_locked two funcs to wait

	void*  : this is always 64bit in linux kernel , and perferred to use unsigned long instead because more explictly
	page table : consists of pgd pud pmd pte and page offset , each have an offset ,so each size represented by PAGE_SIZE , PUD_SIZE PMD_SIZE PGDIR_SIZE
			page tables all implemented by each arch
			those pgd_t pmd_t pte_t pud_t are just unsigned_long type 
		
		we have lots of funcs to  cal the pmd or pud or pte's offset by addr and src_pgd , use those funcs (pmd_offset) , PAGE_ALIGN used to mask into
		the next aligned addr(to page size)
	
		in every type of entry we only use a little of the whole entry ,so others used as extra infos ,
		 in pte , we will have pte_presetn access dirty read write exec user and so on  flags and funcs to check those flags
		we use page table create add free ,flags set funcs to control page tables

	Question: I see PG_dirty on struct page ,and pte entry , so which is the true dirty 
	

	initialization:
		when bootup , memory setup will happen as soon as mem allocation has been decided after arch dependent code
		we first use a simple mem model after boot , then mem_init() use the whole mem model(also set per cpu vars and slab and pageset)	

			
				
		arch dependent:
		 	setup_arch() arch dependent . used to require bios to get memory can be used , 
			then read the cmdline param to adjust that , 
			then setup a bootup mem allocate machine
			then setup per cpu vars , 
			then setup zonelists 
			then setup the true mem management
			then per cpu pageset( the bach size ,hot cold page list)

		zonelists:
			use build_all_zonelists , we initial all zones in all nodes (in uma we only have one pg_data_list
			we initial zonelists with zones from all nodes and sorted by their node numbers , 
				add their zones after previous ones to form a priority when local node's memory not enough
			
NOTE:			zonelist struct of each node after 2.6.32: have at most two zonelists , normal only one (arranged by zone types), if config numa,
				will have two , and the second list used to contain only local zones in order for local allocate specified by GFP_THISNODE
			
use /proc/iomem  or System.map to check system memory 
		
		kernel mem mapping:
			the kernel mem mapped to the 4th gb in ia32  and start at 1MB place , 
			on amd64 , only use 48bit address , so 0x0000 0000 0000 0000 0000 to 0x0000 7fff ffff ffff for use space,
								0xffff 8000 0000 0000 0000 to 0xffff ffff ffff ffff ffff for kernel
			space
			on 64bit ,after kernelspace mapping , we mapping kernel code and then module addrs

			mapping detail:
				first directly mapping to 896mb from 5kb , then vmalloc mem , then persistent map(highmem) ,then
					fixed map(kernel used but not immediately from bootup ,so here but is fixed)

			in 64bit, kernel start at 0xffff 8000 0000 0000, and PAGE_OFFSET from that , fist directly map 2^46,then vmalloc , then vmmen,used by disconfigous mem model,
				then kernel code data , at last module_vaddr used to map module

		detail in SETUP_ARCH():(diff: in ia32 we first setup bootmem allocator then map physical mem to virtual for kernel,
					but on amd64 , we first mapp in(init_mem_mapping() ,then activate bootmem in contig_initmem_init , we discuss ia32)
			first use arch dependent method get memory distribution map(setup memory region)
			then add all mem areas to  struct pgdat_t, (add_activate_range(),use struct max_zone_pfns  and node_activate_region)
			then setup bootmem  allocator
			then stup kernel page table and mem maps ,(paging_init(),pagetable_init())
			then set up the memory list you can see(use free_area_init_nodes()

			in page_init() (mapping physical to virtual and open page stratety,flush tlb, )
	******		then free_area_init_node use zone_pcp_init() to init pageset for cpus

		the bootmem:(use setup_mem() in 32 or contig_initmem_init() in 64)
			use  bootmem_data_t to contain all fragments of physical pages we have ,and maintain a bitmap,
				every time you want mem , it will allocate ,with first match perferred
			it first add checked mem into boot_mem_t struct (linked by list),then search those areas can be used.
			
			usage: alloc_bootmem , alloc_bootmem_pages	alloc_bootmem_low alloc_bootmem_low_page(used by dma alloc)
				use free_bootmem to free allocated mem , 
				use free_all_bootmem_node to pass mem management to brother system

		init func and data clear:
			after boot stage , funcs or datas marked with __init __initdata will be freed

		buddy system : initialize after the bootmem allocator , represented by every zone with a struct free_area(an array of MAX_ORDER members)
					each member is a free list array( linked by list of every free block's first page) 
					now in order to max use the mem space not to be too fragment , the freelist of every order will also by split into
					several migrate types.
				all nodes and all zones have their own buddy system , and will be checked by the order of zone 
				in /proc/buddyinfo  /proc/pagetypeinfo you can see pagetype( movable , unmovable, recyclable)
				when allocate ,mem's will be split into such groups of the same size free list,every type has a fallback type list to use when the specific time
					has been used up

				this type movable will be disabled if not enough mem exist , when in build_all_zonelists
				we use a type flag(enum) to mark every memblock , and so buddy systemd know who not to move. and
				when freee  , then  to add to an exist environment(with that flag)

		free_area_init_nodes: used to initialize mem zone and node data struct:
			this will use arch dependent max_zone_pfn and early_node_map arrays to construct the boundary of all zones and nodes into a generic array
			then call free_area_init_node for all nodes to construct all pg_data_t struct 
				in which call use calculate_node_totalpages to  cal all pages on each node, and alloc_node_mem_map to allocate struct page
				free_area_init_core to init the realsize of every zone,and kernel pages and all pages count
					then all percpu cache init and every free area list head of buddy system init


		buddy system allocate:
			alloc_pages get_zerod_page __get_free_pages get_dma_pages alloc_page  , the getXXX  return the virutal addr ,and allocXXX return struct page
			we also can use vmalloc to allocate virtually continue but physically not continued pages ,  kmallo allocate mem less then one page

			when allocate , use gfp(get  free page) flags to specify which type of mem to allocate preferred, use gfp_zone() to get the correct zone to allocate
				also some combination usually used , GFP_KERNEL(wait|io|fs) , GFP_ATOMIC and so on , in gfp.h 
			
			get_free_pages --->(page_address() after return of alloc_pages) , alloc_pages--->alloc_pages_node (this is the base of buddy system alloc)
			__free_page(page) free_pages(addr) , the latter use virt_to_page(addr) to get struct page ,and then call the former then all to __free_pages
			
			the alloc_pages_node will use the specific zonelist the choose the specific zone ,then use buddy system of their own to allocate that order's mem

		zone_watermark_ok : used to check if alloc in this zone of specific order is allowed
				free pages must be above min watermark , and ALSO must have enough big chunk of every order to avoid fragmentation
				the mark is the zone's pages_min pages_high pages_low set by the least must assured page
		get_page_from_freelist : 
			traverse specified zonelist to get the specific zone the allocate in , then check alloc_flags(ALLOC_WMARK_MIN ...) to set the mark
				then call zone_watermark_ok to check , if success 
				call buffered_rmqueue() to allocate

		alloc_pages_node:
			the core of buddy system to alloc page
			first try zones with only harwall and mark low cpuset , if success , return(use get_page_from_freelist)
			second , retry with first kswap on all zones on that zonelist then lower the wartermark and try again)
			third try with  no watermark flag if is allowed(process flag with not allow interrupt) and with pf_memalloc(set by allocator it self)
				if set no_fail , then spinwait and loop retry again the third
			fourth is sleep is allowed (GFP_wait is set) , then cond_schedule() allow others to run a little time , then continue ,use try to free page 
				to recycle some pages ,then again try alloca use get page from free list() , if not set FGP_NORETRY and set GFP_FS , kill a process to get free
				the got free will be enough and restart from first again.
				if not that condition, and allocate length not too long(<=8page) or set nofail , retry from the fourth , otherwise return fail.

			if get free page success:
				in buffered_rmqueue , will check if alloc just one page , if so ,just use per cpu cache pages  in the pcp list (shoud choose right migratetype),
					if multi pages , allocate from buddy system, (if no enough continous page, return fail, if yes , break big chunk if needed and return )
					the allocation from buddy system use __rmqueue() ,it will handle the split of the big page, (the migrate_reserve is the last to try ,but when?)
				if success allocated , use prep_new_pages() to set correct state of all pages .(eg. for huge page),
		when __free_pages(), we use will free into cpu cache if only one page , or free into  the buddy system and find buddy to merge 

		mapping virtual:  vmalloc
			this used to map memory(usually high mem into kernel for use , but not requre continue , so use virtually continuesly but physicly distinct)
			this will allocate the virtual address just after the kernel persistent mapping 
			vm memory is managed by vm_struct , every mapped virtual memory in kernel space in vmalloc space is represented by one this struct.
				contains vm addr , length, physical page struct pointer array 
				when allocate , vmalloc use get_vm_area to get the vm address range , use alloc_page to alloc enough pages . then map them together
					use map_vm_area and construct page tables of kernel
				it will use highmem as possible

			we use vmalloc  vmalloc_32(make sure physical addr 32bit),vmap(use a pre defined struct page array to map), ioremap(this used to map bus io into kernel memory)
				note that in kernel or userspace , we all use virtual addr , but kernel's mostly persistent mapping , but userspace will use page table ,
					in kernel the true persistent space just minus the PAGE_OFFSET , but in user space , must translate by pagetable 
			
		unmap:  use vfree<==> vmalloc   vunmap<==>vmap ioremap  to free mapping , remove_vm_area->unmap_vm_area->free_pages


		persistent mapping (map high mem into virtual kernel space , just after vmalloc):
			we will have LAST_PKMAP  number of pages in this area, the count 1 means mapped but not in tlb, so not yet can be used. so use number is n-1
			use a struct page_address_map to describe the relationship of struct page and virtual addr 
				then use a hashtable to map all struct page pointers to their virtual addr . 
			when you have a struct page , you call page_address(page) , it will check--:
				if low mem , just return the real addr shift from page_offset
				if high , hash and traverse then get virtual addr 
			so in kmap ( which we map high into kernel space , we do:
				if low ,call pageaddress() , if high , we fist get a lock , then page_address() to get high virutal , 
					if null, map_new_virtual (in which we choose a free place to get vrit addr ,then combine the page and the addr),and return the new addr
				the kunmap : if is in highmem , dec the count array to one, (the to zero will be done by flush_all_zero_pkmaps in map_new_virtual, then
					flush out tlb
		kmap_atomic:
			this is used in fix_addr, and has a type as a id, every "type" var of every cpu has one value in that map range , so
					can be map immediately even have been mapped by others
				on 64bit, just not do with highmem . so kmap just return page_address(page) , kunmap do nothing
			NOTE: we want persistent map and  fix map because kernel can not manage directly map over 896MB, if we can , we just directly map them


		SLAB:
			use slab , we can alloc small mems , also keep more data in cache (if just buddy , the same times addr will always hit only specific cache line)
			we use slab to realize kmalloc
			the slab has a memcache struct ,which have three slab list(free used full),then slab struct manage all mem's alloc
