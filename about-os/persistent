disk operation:
	interface:
		status command data register to control
	internal:
		disks each has two phase, which has many tracks,
			which as many sectors
		disk head seek arond track: seek time
		wait for sector : roration time

	internal policy:
		depend on seek and rotation time which is less
	outer  os policy:
		elevator algorithm, avoid starve
		merge request, increase io throughput

raid:
	consistent update problem:
		if raid write is processing and power is down, will inconsistent,
			so a write ahead log is necessary and usually provided by
			raid controller, so no performance penalty incurred
	raid0:
		raid 0 is strip, one strip is chunks orderly across all disk,
		small chunk size increase paralism io, but seek and roration
		time will be the largest one among all disks
		analysis:
			let say disk seek time a ms, roraton time bms, transfer
			speed c mb/s,
			sequence:
				then sequence io time  of p MB: a + b + p / c = e,
				then sequence speed = p / e, 
				 which will be close to disk speed
			random:
				random io  time of q kb(random io usually smal):  (a+b+p)/c = f,
				random speed = p / f,
					which will be very small

		generayly raid 0 is very efficient, but not tolerate to error


	raid1:
		mirror disks, 
		variant:
			raid10 , first mirror, then stirp , like:
				0 0 1 1
				2 2 3 3 
			raid01, first strip, then mirror as a whole, like:
				0 1 0 1
				2 3 2 3
		fail tolerant :
			certain for one disk,
			but is loss in the same mirror group, can to lerate n/2 disks

		analysis:
			sequence write,  n/2 * s(sequence io speed) need write 2 copies
			sequence read,   n/2 * s
				because as ostep said , if we choose read block 0 from
				disk0, block 1 from disk1, block 2 from disk0, block3 from
				disk1, then every disk will suffer from seek and rotation
				instead of truly sequence read, that is just a logical
				contingous but physicaly seperate read, so only half
				bandwidth
			random write, n/2*r (need write two copies)
			random read, n * r(can read from every disk)
				across all disks for read, so full speed

	raid4:
		n-1 disks for io, one for parity, 
			just use xor for each bit as parity, in real is an even parity check	
		sequence rw, (n-1)*s (speed)
		random read, (n-1)*r
		random write:
			1, additive method, read all data from other disk, 
				recompute parity, this is too cost
			2, substractive method, read just the io disk and parity disk,
				check if parity bit need flip or not
			the method2 because parity disk is bottle neck disk for 
			multiple random writes, so must sequence order when actually
			perform, so performance is just R/2, R is random read performance
			of raid0, which is best, 
			this is called small write problem

		write latency:
			2T (which is one disk latency)
			because need two one read and one write on one disk

	raid5:
		just like raid4, but parity roundrobin into each disk in every stripe,
		random read alittle better, across all disks,
		random write (small write) now increase into (n/4)*r 
			because every write incur two read and two write
			but will not bottleneck into just one parity disk

	conclusion:
		if performance, raid 0
		if random and redundancy is important, raid 1 is better
		if capacity and reliability and cost is important, raid 5 is better
			(raid 4 is a subset of raid 5, just implement simpler)



file and directory:
	debug:
		strace  -e trace=open,close,read,write....
				-f trace_forked
				-t  report time

	fsync():
		write always return but data is buffered, so fsync() need 
		to make sure data is on disk, but also fsync(dir's fd) also
		needed if file is newly created, so dis's inode is updated
	rename():
		guarantee to be aotmic, so usual action of add data to file
		is : open a tmp file, write into it, fsync(), then rename to
		the specified name

simple filesystem:
	disk split into blocks(eg. 4k), some for data store, some for inode
		store, which record which file's data in which data block,
		inode table, data table: record which inode, data block is empty
		or inuse, 
		super block: record how many inodes , datas block we have,
			where table and bitmap begin ...

	inode data store:
		first search using inode index number, calculate from start
		and calculate the block number ,issue a read of that block,
		then we have inode info, 
		1 in which is stat of file and data 
			pointers, eg. 12 direct and 3 multiple level index, 
			because most file are small, some are big,
		  this is ext2,3 's format
		2 extent format, store pointers to data and length of data,
			just like cpu segment method in virtualization
		  xfs and ext4 use this

	dir:
		directory just special files containing data as
		its files's inode and name and other things like string length
	open:
		superblock read from disk when system boot,
		so search from '/' 's inode, every inode and data nodewill be 
			read to find the next inode, finally the specified file's inode,

		when create:
			bitmap will be read and write to allocate block for new file's
			inode.
	write:
		new inode allocated by bitmap, and update inode and write to 
		date block
	read
		rw to update inode time and  read to datablock

		read/write will also need update memory structure of position
	close
		nothing to do
		no need to fsync()
	fsync
		make sure buffered data flushed to disk
			
	these are expensive, so modern os have filesystem cache of read and 
		write, after some seconds then data will be flush, but will suffer
		losing data, so database sometimes work around this cache

fast file system:
	introduce rename, symlink
	most important:
		block groups, because disk seek cost too much, one groups
		combines some cylinders, all files in  one directory and its
		data block allocate in one group as possible, if file too
		large, the indirect index's put in another group

	log:
		data journal:
			first wirte into a journal special area, then write into 
			actually disk filesystem, 
			good:
				assure data consistent
			bad:
				write two times
		meta journal:
			data write to disk first, then write journal, then journal
			write to disk
			good:
				only write one time of data, which is big
			bad:
				if crash, data may lost

log-structured fs:
	used to optimise seek time when write, because os has cache,
	read is generally cheap
	this is a append-only system, every write, will write a segment,
	(one segment may be many writes combined together),so seek time
	is very small, and new inode, new bitmap also as data in new segment,
	and keep a checkpoint region at a fix point for latest bitmap places,
	every time read, just read the latest ones, 

	garbage collection:
		periodically need check some segments for data block if not live,
		free them by append new collected live blocks, free old ones for
		future use

date protection:
	1 sector lantent:
		sector just destroyed by disk head, thus return error when read
		or write, so raid or other methods used to remedy data from other
		disks
	2 data corruption:
		1 mis direct write:
			include parity check in every sector for data and which disk,
			which sector
		2 loss write
			include check both in disk and in inode, so when read, check
			if they are the same
		when to check:
			check need space, but limited, os check may happed when copying
			data, so is efficient,  disk check may happen every period of 
			time when every week for eg.

distribute file system:
	client talk to server using udp or tcp(with acknowledge)
	rpc:
		client send in a stub code,which sent across rpc library,
		server receive and decompile from stub, then run method,
		then return
		often use udp, and do relabile check itself instead of tcp

nfs:
	use a stateless convention, every open, read write close will
	contain every info that operation needs by server, so crash
	only need a client retry(no keeping file descriptor inclient)

	cache:
		client cache data and have a local attr cache which update
		every some seconds, every time retrieve data, client will
		ask attr cache if update to data, if not ,just fetch again
	write back on close:
		every time close a file , client will write to server the 
		buffered write data, for other client to get newest data.

afs:
	once open , download all file data from server and cache in local
	disk, also register callback along path-to-file each directory,
	once close file, just flush to server, then server notify from callback
	to all other clients to fetch new version of data

	when crash:
		client: grab new version of data from server or check stat for 
		if local save is uptodata
		server: notify clients to refetch data again
