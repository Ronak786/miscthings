namespace: 
	in a namespace , other things outside can not interfer with it, used by virtual machine as a container
	
memory space:
	virtual memory in 64bit will not all be used , to just 40~50bit is enough , and save cpu overload ,
		so have holes in memory pagetable

into kernel method:
	user space syscall
	interrupt from hardware
	kernel threads being run by schedule? ps will show like [XXX/1] means XXX threads run on cpu 1

page and tlb :
	page have herichy , PGD PMD PTE offset , tlb will be flushed every time page table changed , cpu do it or kernel 
		do it ----arch diffs
	
	kernel will always assume  four level page table , sometimes ignore some level , but it is the kernel portion related
		to arch do that , the general part assume four

memory mapping:
	kernel maintain the mapping , others can just access memory , the change will be auto sent to device or file

memory alloca:
	brother system : 
		kernel have a lager contigous chunk of memory, when needed a small one , break into half ,
			if just between half and double half , alloc to him. if not ,continue to break to half,
			then when recycle , if address is contigous , merge to double.(but if frenquently allocated,
				can make scattered )
	slab : 
		this is defined above the brother system , can cache a chunk of memory to allocate small memories
			for structures and so on,  using for example: kmalloc kfree 
	page fault and recycle:
		swap be used to store tmp not used pages , recycle is when memory needed , flush data and recycle
			pages , just mark as page fault on the virtual page(physical page recycled)


system call :
	user to kernel , defined by posix some

net:
	socket is a proxy between user space and kernel .

buffer:
	page cache(cache always in pages .so page cache(
	buffer cache now is little

list:
	list_entry(ptr, type , member ) to get the struct type addr which contains the struct list member pointed by ptr
	list_for_each(tmpvar, &list_head)  will in a for loop return every host struct

kobject:  export to sysfs to manage objects in system(mostly devices models)
	used to represent device models . each object is contained inside the managed object ,
		kset:and some object can become a set , the kset pointer point to the root kset of them
		ktype : a set's common interfaces
		kreference: the object's reference count
		list : all object included in a set
		uevent_ops : funcs about uevent
		

kernel data types:
	1 , typedef __u8 __s8 ...
	2 , cpu_to_le64  le64_to_cpu
	3 , per cpu type , get_cpu(name,cpu)
	4 , user space pointer: use __user identifier

now procceed forward!!



schedule process:
	determine time the process can wrong and how to context switch

processes type :
	hard real time :  not exist in kernel
	soft real time : will suffice as soon as possible
	nornal : scheduled by cfs ...

schedule policy:
	a timeslice , preempted , means when time over , scheduled out 
	O(1) ,CFS the latter will try to give every process a fair time(or even every schedule entity)

process state: <depends on the preempt option on the kernel config>
		running ,waiting(can run but have no cpu) , sleeping , terminate , zombie(terminated but not reaped by father)
		schedule will happen only from leave interrupt or from into userspace , 
		kernel threads also can be preempt 
		interrupt can always preempt others
	
		in task struct ->signal struct -> we have limites (getrlimit setrlimit )used to manipulate
			process's system resource usage	
			also in /proc/self/limits

fork exec clone
	fork then exec , the clone can do like fork but have a finer grained operation

namespace
	can use unshare() to implement , kernel must implement too
uts namespace:
	about the arch and kernel names in diff view , allocate a new structure

pid namespace:
	used for a process to isolate from parent , have a unique overview of the system global pid resources.
	new pid namespace will have new pid 0--init process(but is a normal process in the parent's process view)

INFO:
	in a pid namespace , one task struct(one process) will have many ids in many levels of namespace,
		so it use a struct pid_link(a struct pid) to represent all these(with level and name space as param)-relative
		to one process in diff namespaces(only when in threads , one thread exec and replace all threads in 
			process , then two process(thread) may be have one struct pid
	but one struct pid also will contain lots of taskstructs,when have the same pgid sid , thread id

	1) now , if you have task struct , id type , name space , you can get upid through pid_link,then struct pid ,
		then use level to get the upid
	2)from upid to task_struct:
		every upid is scattered into a hashlist(the var in struct upid) , use namespace and upid to 
			find that struct upid in the hashlist , then find struct pid , then find task_struct(
				usually have only one process in the tasks[TYPEPID]
		in the find_task_by_pid_ns() , provide namespace and upid , it will search the global
			pid_hash[] array(in which is an array of scatter lists,use that hash number to choose one
			list then traverse) to get the struct upid

struct pid:
	we use struct pid to manage all kinds of ids ,pid pgid ptid  sid , because these ids just make 
		some processes to get their place at leader_group->pids[types] (hash list)
	use attach_pid to attach the ip of type from the struct task to the struct pid

alloca:	everytime a new process created in the current namespace , all local pids will be allocated by
		alloc_pidmap through alloc_pid to allocate to sevaral namespace
	new made  upid structure will be scattered into  pid_hash[]


about task struct:
	the group leader is the leader thread of the thread group ,getpid() will return this process's pid
		so the tgid is that pid
	the children and sibling are two link lists linked to brothers(same parent) and childern
	a thread_group list  link all thread of one process(group) as together 


about fork:
	when fork ,copy on write , means: all page tables marked readonly ,when one of pros about to write ,
		exception the kernel  ,kernel check other memory structs see if allow write ,if so ,copy a self copy
		and allow write on both ?

	when fork , system do the sys_fork , 
		args: pt_regs is a structure be pushed in by kerenl when interrupt happens , store
			all register's value ,so system call can get value
diff fork  clone:
	if use fork ,flags are fixed , so not use registers to pass params . 
	if use clone , flags passed from user space , so will use those params from registers 
		they are pushed into the structure by the kerenl  when interrupt system call

	in fork , all other things done by do_fork, which will copy process , then if vfork ,wait
		for child to complete(use wait_for_complete),if not ,just wake up(added into 
		schedule queue ,and mostly will be scheduled to run)


ERROR  INFOS:
	in kernel , if one func return pointer as retval , but if error , will not just return a errno value , so use macro  ERR_PTR to map that
		errno to a pointer value between 0 to 4kiB which will not be used by virtual memory

in do_fork:
	we copy_process , allocate pid , check vfork or just fork and wait or not

in copy process :
	check flag combinations , eg . CLONE_NEWNS & CONLE_FS not mean(one to allocate new filesystem , one to share)
					clone_thread should along with clone_sighand to allow share signal(or not)
					clone_vm then you can share signal handlers
	in task_struct->stack , is a thread_info struct unioned with a stack(always two pages)
		the thread info also linked with task struct , and has preempt count and other arch specific infos(cpu,sigpending ,need_sched),vir_addr_limit ...
			the thread means arch dependent , not that thread!!!
	the current macro  devrided from  current_thread_info

	after copy , we set initial vars in struct ,set schedule stats , then according to flags , mark corresponding resources for share
	the last copy_thread copies arch dependent statistic(the thread_struct in the task_struct)

	then set all id's relationship , sid pgid pid  tid, specially when encounter the CLONE_THREAD


SOME CLONE FLAGS:
	CLONE_CHILD_CLEARTID : when thread destroyed , will put 0 in a user space passed in , then wake up userspace , can be used like pthread_join() ?
	CLONE_CHILD_SETTID
	CLONE_PARENT_SETTID
	
kernel threads :
	begin with kernel_thread, register and run a kernel thread
	but we now more use kthread_create()  kthread_run() , more moden and can select cpu number to run

	kernel threads will use active_mm pointer to point to mm_struct of the former struct , just effectively use , not flush
		but now mm pointe is NULL , to distinguish. then if the next to run is the same as former , we can just switch and not flush tlb ,good job!!


scheduler:
	every process may be shceduled by cfs or rt-scheduler, we have static priority(one can be set) and dynamic pri(calculted by kernel from static one)

	all processes are in all cpus's runqueues , every process in just in one cpu's runqueue(one cpu has one runqueue ,defined in kernel/sched.c)
		in the runqueue , all schedule type have one struct of sched entity for each type of scheduler(rt , fair)

	process has three kinds of priority values: (we have a kind of pro that usually normal ,but be level upped by kernel to the level of read priority , that is represented by prio)
		prio , normal_prio, static_prio
		static_prio : is the value got from nice or when task is created  (this value must above 100 ,so is always in the domain of normal priority)
		normal : if not real time process -> just static , if real time-> 100-1-rt_priority (in kernel , priority smaller , higner )
		prio: if normal ->static_prio .  if not normal , just not change 
			when forked , child's static pri is father's,  child's prio is father's normal prio

	LOAD WEIGHT:
		the load weight is: weighed by nice value , when inc or dec by 1 , the weight value is changed by 10% , all values are 
			in an array , rt process have twice the value of the highest value of the nornal value 
			and the runqueue's weight is just the sum of all process in the queue 's weight

two kinds of scheduler:
	periodic scheduler -> the scheduer function called which tick , so called schedule_tick() , it will update runqueue's clock ,then check if current process(
		get from runqueue struct) run enough time . then set tid_need_sched flag in thread_info struct .  when return from interrup , kernel will check that
		flag and schedule() if.    

	main scheduler-> when a process want to yield to others , then it call schedule() . (all funcs prefix with __sched will call schedule, this tell link to put then
		in a special seg so dump will not show them ) .   
			or when return from syscall , kernel will check it , no matter either , it is in process's kernel space , so next time run ,just after that place in the 
				scheduler() function
		it will update clock and clear the flag ,then(if signal happen,again wakeup pro, otherwise) dequeue task and record something then choose new task
			then do the context switch ,then test if need_sched(the new pro's flag) , if so,continue to schedule from head)

	schedule in fork:
		in fork , we have a sched_fork() to set some schedule-related vars in struct , then set prio of child to normal prio of father , 
		when in wake_up_new_task , task will be queued


	in context switch:
		we do the memory switch ,then switch the stack , use switch_mm() switch_to() , 
			in switch to, we complete the switch , so subsequet codes will be done 
			the next time this process be scheduled to run



CFS:
	above we know priority mapped to fixed load weight in every sched_entity .
	all time and weight's update is in the update_curr() func,
	update_curr:
		update the pro's exec time , 
		calculte vitural run time of this time ,(if nice is zero , just is a real time (now-exec_start))
			it will be a fraction of the realtime-delta, nice0_value / nice_now_value(weight)
			then entity's vruntime added by that ,as the new vruntime.
			then calculte the min_vruntime  of the cfs_rq runqueue , this should be bigger than 
				its old value ,and may be bigger than the least value in the tree(because it is
					compared to that to decide the new value) ,it is one of the current vruntime , 
					old min_vruntime , tree's least vruntime
			then the entity's keyvalue is vruntime - min_vruntime:
				means key will expand according to runinig time inc , high priority's pro will
				increase slowly , if sleeped , after wake , due to min_vruntime increase(mono inc)
				the key will be smaller than before in spite of the non-changed curr->vruntime

		in cfs , we also has a schedule lantency , in which time ,all process should be run at least once ,
			that is set as a default value and can be changed by sysclt_sched_latency and sysctl_sched_min_granularity,
			the physical slice is allocated by this depend on load weight , then virtual one depend on that
			(why time slice again ???) (this is used as a measurement, as a ideal run time , if real run time 
				is above that , then need_resched)

		about enqueue:(enqueue_task_fair)
			two states , not wakeup(is running just before) , wakeup(enqueue from sleep or newly created)
				if not just wakeup , directly enqueu into the rbtree(vruntime is valid)
				if newly created(inside in type of wakeup),add a lantency,so will run just after 
					the current period of all process's run
				if from sleep wake up , so the vruntime may have been too small ,(so will keeped be 
					chose to run , so shoud modify it to a value not too old , so choose 
					the max of the se->vruntime and vruntime-lantency ,(kind of fair?)
		pick next task fair:
			see if has process to run , then pick leftmost , dequeue ,save on the cfq_rq->curr ,
				save exec_time , and start record from now on

		check_preempt_curr(check_preempt_wakeup in cfs):
			process who call try_to_wake_up , in the wakeup routine , func will check if the newly
			waked up process can preempt , 
			the check:
				new vruntime bigger than current , just return , then do current
				new smaller than current , and new + granularity < current , then resched
				otherwise do current?
