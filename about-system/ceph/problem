iozone's direct on client is better?  
block size bigger than 512k 1024k or more ???
	the true bottle neck is wirte into one swgfs file, but if using too many  threads to 
	write, will cost more(thread sum: 7 work, four rw file ??, or I should range thread sum
	into less than cpu num, so while open thread, check, so global thread not a good idea)

update scst 3.2


_FILE_OFFSET_BITS ==> __USE_FILE_OFFSET64
modify /usr/include/sys/statfs64.h struct statfs {}  signed long into unsigned long

sg_ses --dev-slot-num=0 --clear=fault /dev/sg14

/usr/bin/ceph-mon -f --cluster ceph --id x86_storage1 --setuser ceph --setgroup ceph to start ceph mo instead of systemd

look at the ceph/guide of gentoo !!!!
ceph osd too less: ceph osd pool set rbd pg_num 128

remember to touch /var/lib/ceph/mon/ceph-node1/{done,systemd}  !!!!

osd name too long:  osd max object name len = 256  should use xfs instead of ext4
					osd max object namespace len = 64

all owner should be ceph !!!


remove cephfs:  /etc/init.d/ceph stop mds ,   ceph mds fail 0{1,2,3,4,...}  ,  ceph rm fs xx --yes-i-really-mean-it
				then remove pools

ceph osd crush rule create-simple rule-name  root-name   bucket-type   ==> to create a rule
ceph osd pool set poolname crush_ruleset [num] to assign rule to a pool
每个pool的pg数量如果过多会导致问题，一个pool对应的osd群在crushrule里设定，所以可以适当更改规则



创建snap的时候，原image必须没有exclusive lock， 不然之后是不能恢复的  会只读
创建文件系统的时候，制定权限时，要具有对mds的读写权限，这个控制对文件系统的读写与否
权限控制：
	ceph-authtoool可以定制到文件里，但最后需要ceph auth add name  -i  file  来读入系统中才能生效，
		每次修改权限都是一次重写
