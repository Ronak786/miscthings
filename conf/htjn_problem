p1
admin 10.10.5.211
scsi-sub 10.10.5.0/24
scsi-sbu 10.10.6.0/24
nodebackend1 10.10.5.211
nodebackend2 10.10.6.211

p2
admin 10.10.5.212
scsi-sub 10.10.5.0/24
scsi-sbu 10.10.6.0/24
nodebackend1 10.10.5.212
nodebackend2 10.10.6.212

p2
admin 10.10.5.213
scsi-sub 10.10.5.0/24
scsi-sbu 10.10.6.0/24
nodebackend1 10.10.5.213
nodebackend2 10.10.6.213


gluster vol create gfs-vol replica 3 10.10.5.231:/opt/petasan/config/gfs-brick  10.10.5.232:/opt/petasan/config/gfs-brick 10.10.5.233:/opt/petasan/config/gfs-brick force

gluster vol set gfs-vol network.ping-timeout 5

gluster vol set gfs-vol nfs.disable true 

systemctl start  petasan-mount-sharedfs

10.10.5.10/ui vsphere@htjn5F
wifi htjn666.888
192.168.2.141:8080/vgs pass1234

=============================TECHNIQUE===============================
grep '\--cluster' -R  * 
	used to send  get ceph api in petasan


==============================PROBLEM=================================
parted -s /dev/nvme0n1  mklabel gpt mkpart primary 1M 25G  mkpart primary 25G 30G  mkpart primary 30G 55G mkpart primary 55G 60G mkpart primary 60G 85G mkpart primary 85G 90G
when install, careful the Time!!!
stop ntp service, use ntpupdate to udpate time

1 can not set same section of gluster and management
	/opt/petasan/scripts/node_start_ips.py  , ignore return false when subend1 ip failed set(already exists)
	if can not work, using vlan and try

2 gluster vol set gfs-vol  nfs.disable true have multiple
	in usr/lib/python2.7/dist-packages/PetaSAN/core/cluster/sharedfs.py
	
3 clock drift allow too small
	mon_clock_drift_allowed = 2
	ntpdate your ip (configure ntp server first)
	hwclock -w 

4 mds install
	dpkg --ignore-depends=ceph-base -i ceph-mds_12.2.7-1_amd64.deb 
	dpkg -i ceph-fuse_xxxx.deb

	modify conf, add mds, see gentoo ceph install
		[mds.p1-0]
		  host = host1
		mkdir -p /var/lib/ceph/mds/demo-p1-0
		ceph -c /etc/ceph/demo.conf auth get-or-create mds.p1-0 mds 'allow' osd 'allow *' mon 'allow rwx' > /var/lib/ceph/mds/demo-p1-0/keyring
		chown -R ceph:ceph  /var/lib/ceph/mds
		cp /etc/systemd/system/ceph-osd@.service.d/override /etc/systemd/system/ceph-mds@.service.d
		systemctl enable ceph-mds@p1-0

	create pools:
		ceph -c /etc/ceph/demo.conf osd pool create cephfs_metadata 64
		ceph -c /etc/ceph/demo.conf osd pool create cephfs_data 64
	ceph fs new cephfs cephfs_metadata cephfs_data
		

mount feature:
	if feature not supported very well, will not mount, check kernel's include/linux/ceph/ceph_features.h to see
		which feature you lost
		you can set crush type using ceph --cluster demo osd crush tunables hammer(0.94) for 1 << 58
		for kernel 3.13 : 1 << 48  CEPH_FEATURE_CRUSH_V4 
						1<< 41 CEPH_FEATURE_CRUSH_TUNABLES3 
			mon0 10.10.5.211:6789 feature set mismatch, my 4a042aca < server's 40102004a042aca, missing 401020000000000

cxgb4 bin from linux:
	use 1.20.8 instead of 1.17 from chelsio 3.6.3

test kernel from http://yum.oracle.com/repo/OracleLinux/OL7/UEKR4/archive/x86_64/index.html
	4.1.12-32.1.2.el7uek.x86_64 default mpt3sas must modify bad
		dracut --add-drivers=mpt2sas --omit-drivers=mpt3sas name.img  version_kernel
	4.1.12-37.2.1.el7uek.x86_64 default mpt2sas can use		bad
	4.1.12-94.1.8.el7uek.x86_64  bad ,must manually add mpt3sas bad
	kernel-uek-4.1.12-103.3.8.el7uek.x86_64.rpm   mpt3sas  bad
	kernel-uek-4.1.12-112.14.1.el7uek.x86_64.rpm  bad
	kernel-uek-4.1.12-124.14.1.el7uek.x86_64.rpm bad
	4.1.12-124.14.3.el7uek.x86_64 bad
	4.1.12-124.14.5.el7uek.x86_64
	4.1.12-124.15.1.el7uek.x86_64 good
	4.1.12-124.16.1.el7uek.x86_64 good
	4.1.12-124.18.1.el7uek.x86_64   good
	4.1.12-124.26.3.el7uek.x86_64  no problem
	4.14.35-1844.3.2.el7uek.x86_64 default mpt2sas can not use, modify ,  good

kpatch:
	kpatch-build  -s /root/rpmbuild/BUILD/kernel-3.10.0-514.el7/linux-3.10.0-514.el7.x86_64/ -v /usr/lib/debug/lib/modules/3.10.0-514.el7.x86_64/vmlinux   /tmp/list
	require: kernel-debuginfo rpm, src rpm

in sw64:
	bluestore.cc should use -O0 to avoid optimize problem ,and .got toobig, split into two files
	lru_cache.cc should use gcc flag -mieee to avoid underflow problem

bcache:
	need a new bcache module from htsw's bcache.ko, should change name from vgacc to bcache

htsw vgs:
/usr/local/vgs/tomcat/webapps/vgs/view/login.html   uncommented all js code for authencate


======================================== ceph install ================================
time sync:
	chrony, use this!!!, config is chrony.conf

config:
	https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.2.3/html-single/ceph_configuration_guide/index
system sysctl optimal is in gentoo wiki last line!!!
manually:
	https://wiki.gentoo.org/wiki/Ceph/Guide for 12.2.11 version install manually
deploy:
	http://www.yangguanjun.com/2018/04/06/ceph-deploy-latest-luminous/
	first install chrony
	disable firewall
	install new nopassword user as official guide
	set hostname
	install lvm2
	install according to guide, but need to zap disk if has gpt label
need: chrony enable
		lvm2 enable for ceph-volume  detect when startup
option:
	ceph osd set noout ; forbit reboot too long inducing out and pg migrate for small test clusters

remove:
	ceph osd out osd.1
	ceph osd rm osd.1
	ceph osd crush rm osd.1
	ceph auth del osd.1
	export num=4; ceph osd out osd.${num} ;ceph osd down osd.${num};  ceph osd crush rm osd.${num} ; ceph auth del osd.${num} ; ceph osd rm osd.${num}

rm pool dynamicaly:
	ceph tell mon.\* injectargs '--mon-allow-pool-delete=true'
	ceph osd pool rm test-pool test-pool --yes-i-really-really-mean-it
	ceph daemon mon.n2 config show | grep bluestore_cache  to catch config
	ceph tell osd.* config set bluestore_cache_size_hdd 3221225472 to set config
enable app:
ceph osd pool application enable <pool-name> <app-name>', where <app-name> is 'cephfs', 'rbd', 'rgw', or freeform for
custom applications

cxgb:
cxgb4/adapter.h

osd pg distribution:
ceph pg dump | awk '
BEGIN { IGNORECASE = 1 }
 /^PG_STAT/ { col=1; while($col!="UP") {col++}; col++ }
 /^[0-9a-f]+\.[0-9a-f]+/ { match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;
 up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)>0) { osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) }
 for(i in osds) {array[osds[i],pool]++; osdlist[osds[i]];}
}
END {
 printf("\n");
 printf("pool :\t"); for (i in poollist) printf("%s\t",i); printf("| SUM \n");
 for (i in poollist) printf("--------"); printf("----------------\n");
 for (i in osdlist) { printf("osd.%i\t", i); sum=0;
   for (j in poollist) { printf("%i\t", array[i,j]); sum+=array[i,j]; sumpool[j]+=array[i,j] }; printf("| %i\n",sum) }
 for (i in poollist) printf("--------"); printf("----------------\n");
 printf("SUM :\t"); for (i in poollist) printf("%s\t",sumpool[i]); printf("|\n");
}'

remove
ceph-deploy purge
ceph-deploy purgedata
apt list | grep '12.2.7' | cut -d '/' -f 1 | xargs -n 100 apt purge -y

========================================= source config ======================================
sw apt:
	202.141.160.113:8081
a dir has all debs(of multiple level if you want
nginx to export that dir
if no Release, then use https://medium.com/sqooba/create-your-own-custom-and-authenticated-apt-repository-1e4a4cf0b864
	to create or just use deb [allow-insecure=yes] to ignore
use dpkg-scanpackages to creat Package file for index
	dpkg-scanpackages  ../../pool/main/  /dev/null > main/binary-sw_64/Packages

======================================== test ===============================
4.4 disable feature of tunable
client:   933 MB/s wr, 0 op/s rd, 245 op/s wr
client 4.17 tunable hammer/default 116 MB/s wr, 0 op/s rd, 31 op/s wr


tshark:
	tshark -i enp9s0 -f 'host 192.168.2.0/24' -w /tmp/capture4.pcap


==========================================  build deb ceph ================================
apt install curl libcurl4-openssl-dev  libkeyutils-dev keyutils libldap2-dev libaio1-dev python-sphinx python3-sphinx 

nohup dpkg-buildpackage -j8 -nc -uc -us  -b  > ../build.log &



==========================================  web related ===================================
hava node statistics: opt/petasan/scripts/node_stats.py
throughput: storage-appliance/usr/lib/python2.7/dist-packages/PetaSAN/backend/cluster/benchmark.py

========================================== wireshark ======================================
wireshark -k -i <(ssh root@remote-server "tcpdump -s 0 -U -n -w - -i eth1 not port 22")

========================================== how data get ===================================
1 from scripts/node stats.py, all about node stats:  directly send to carbon(graphite) by nc periodicly
2 from collectd, about ceph data, in opt/petasan/scripts/stats-setup.sh, and plugins copied into /usr/lib/collectd/plugins
3 graphite: carbon listen on 2003 used to get data from 1 and 2,  whisper-fetch used to reterieve data 
	and send them to grafana
see https://stackoverflow.com/questions/25651902/is-it-possible-to-query-data-from-whisper-graphite-db-from-console
