SYNCHRONIZATION ISSUES
	lock   scalable clean , must be thought when start design , not after work!!

	ensure unsafe concurrency  and race not happen	, so make critical regions a atomic instruction.
		so we use lock , we have diff types of locks , diff in behavior when lock can not be held
		and the lock code itself is atomic assured	
	user space : pseudo concurrency , multi-pro  , true concurrency
	kernel space: interrupt  softirq tasklet workqueue ,   preemption ,  smp (sleep in cirtical is not allowed)
	
	lock should design beginning ,not afterthought
	we have  smp-safe preempt-safe  interrupt-safe

	the smp and preemption will add lock code to kernel , if not CONFIG_SMP  CONFIG_PREEMPT , those will 
		disappear ,  so choose when needed

	MUCH TAKE CARE OF  global data and share data between interrupt, smp , thread , process , func

	
	DEAD LOCK
		do not lock twice , 
		lock in order
		lock as simple as possible
		make sure procession will finish,so lock will be released absolutely

	lock is needed by more and more scalibility system
	locks begin with a big kernel lock , evolving into nowadays fine-grained lock ,
		but should not be more fine-grained , will add cost to lock overhead
	start simple and grow in complexity only as needed , simplicity is key
	
ATOMIC
	we have atomic integer  and atomic bitwise  implementation , 
	test_bit is atomic implementation , __test_bit() is non atomic

SPIN LOCK
	spin lock will busy waiting , always should disable interrupt , spin_lock_irqsave/irqrestore , 
		(note this lock will disable preempt)
		spin lock can disable interrupt  , bottom half (have some type of funcs)
 
	rw_lock can be acquired(spin lock implement),  
		read can be just acquired in interrupt , but write must disable interrupt
		kernel prefer  read  than write!!
	
		note:  also have rw_semaphore  ,this diffs that can sleep
			and both use uninterruptible  versions' read & write,
			have a downgrade_write to down write into read

	semaphore  can sleep , but have much more overhead ,so short  duration should use spin lock
		 only used in processor context (sleep ) ,can not acquire when having spin lock,
		 , also not  disable preemtption

		we use  down_interruptible  down()  up()  down_trylock sema_init(), to control
			the interrupt means when wait for lock to acquire , can accept signal
			and return fail to get
NOTE:   DECLARE_MUTEX   init_MUTEX used in semapthore
	DEFINE_MUTEX    mutex_init  used in mutex  !!!
	ofter mutex  better than semaphore

	mutex   
		mutex defined for effecient , 

		mutex must be get and release by the same processor , 
			can only be used in processor context ,
			more stricter rules  in Page 196

		
	compeletion :
		DECLARE_COMPLETION()   init_completion  wait_for_completion  complele() 
		when called complete() ,any processor waited on the complete var will be signaled up	

	BKL  big kernel lock
		used only in process  context , 
		can be auto released while sleep ,
		will disable preempt
		use lock_kernel()  unlock_kernel()  kernel_locked()
		
		used when early in kernel 2.0to help smp , now should not used in new code
			mostly used in sync func calls

	seq lock:
		this can be used in interrupt context,
		write_seqlock() write_sequnlock() ,  
		but read in this form
		do {
			read_seqbegin(&xxx)  // the lock
			xxxxx //do some thing simple
			
		} while (read_seqretry(&xxx))  //means the number read now diffs from the number in read_seqbegin
			// so this time's read mixed with write , so must loop reading again
	
		this is lightweight , when have a lot readers , few writers ,data simple , 
			writer never afftec readers' numbers 


	kernel preempt:
		can be disabled enabled by preempt_disable  preempt_enable , also nestable , preempt_enable_nosched()

	barriers: (compiler knows current context's reorder or not , but interrupt happens or not ,don't know)
		mb()  rmb()(read)  wmb()(write) , memory barrier used to set barrier meaning that (load/store/all)
			operations before the barrier will be done before those instructions after the barrier
		smp_rmb  smp_mb smp_wmb  smp_read_barrier_depends() , barrier()(prevent compiler from reordering)
			 these will be memory barriers in smp , and compile barriers in up(uni processor)
		read_barriers_depend() ( this is faster)

		compiler , processor will  reorder two instructions , so in kernel , barrier is important
			eg.  a=1 ; a = 3 ; b =a , now b == 1 or b==3 ? 



TIME MANAGEME
	the time between two interrupt handlers is the tick , which is 1/(tick_rate) , kernel use this to measure time	
	when interrupt issues , update system time , boot time , dynamic timers' count ,balance scheduler's queue , update
	processor statistic and  resources

	kernel notion of time is  from the  system timer ,which update periodically by the interrupt , with frequency of
		HZ ,which is defined by CONFIG_HZ(usually 100 ~ 1000)
	with resolution grow up , accuracy  grow high
	
	the poll select() will waste less resource waiting
	the scheduler will be more balanced ,(if hz is small , tick time is high , then schedule lantency is big)

	TICKLESS 
		a tick one will interrupt every tick, but tickless one will only issue interrup when need some work to do,
			when totally idle , will not tick any more!! saving power and system overhead,
			but will add some expense between userspace and kernel space

	 	a jiffy is a tick , between two interrupts' time  
		 	it is type unsigned long ,and is volatile , so will not be optimized

		jiffies define  32bit and 64bit diff , but occupy same memory , 32bit is low bit of 64bit,
			in 32bit, we have special func to access 64bit jiffies64 , but 32bit is enough ,
			in 64bit , we will always access  64bit jiffies== jiffies64


	system have a rtc ( maintain the wall time, mostly for user to know time of world)
		also have some system timerses ,the main is system timers
		every tick ,the system timer will trigger the time interrupt  handler , 

		detail : interrupt handler
			update jiffies , update wall time , update process time statistics ,
			update schedule issues , monitor whether local timers have expired ,
			set the softirq to do local timer funcs in bottom halves

		we can use init_timer , add_timer , mod_timer to init , add or modify the added timer ,del_timer_sync to 
			delete timer and wait for the already running handler to stop

	we also have no system timer  delays. (just processor block for a little time)
	1 just busy looping the specified jiffies (silly!!)
		we also can use cond_resched() to instead of busywaiting ,but never use some func that can block or sleep
	

seconds :  1 second = 1000 milisecond(ms) = 1 000 000 nanosecond (ns)= 1 000 000 000 microsecond(us)

	when we really need smaller than 1 tick's delay , we can use  udelay(micor)  ndelay(nano)  mdelay(mili) ,
		they busy waiting , so should use with very small duration

	about bogomips , bogomips be calculated when boot , and be used by udelay to calculate how many iterations 
		should be used

	use schedule_timeout() to sleep a specific time and then waked up by kernel , (so should in process context 
		and not hold a lock) ( the schedule_timeout will put process sleep for sometime and wake up it / or waked
			up by interrupt)


MEMORY MANAGEMENT
	32bit and 4kb pages , 64bit has 8kb pages

	struct page :  used to describe physical page(every has one!!),not virtual pages , so according with swaps and mappings , 
		they will associate with diff virtual pages ,
		member:  virtual --  virtual address
			 count  --  page refernce count

	kernel divide memory into some zones , depend on diff archs , we have diff zones containing diff memory areas
		ZONE_DMA  ZONE_NORMAL ZONE_HIGHMEM
 	we use struct zone to represent zones
	
	the HIGH MEMORY problem:
		in old days , when 32bit , when you use more than 4gb memory, kernel can not map them all in 
			virtual memory , so high memory(a portion in kernel space) be used to dynamically 
			map them when needed 
			but in 64bit ,available vm is too much , so high mem no more needed

	when allocate , we use alloc_pages(return page struct) as a base low func , 
		above , we have alloc_page , __get_free_pages(return logical addr) , __get_free_page , 
			get_zero_page ,kmalloc(alloc physical&virtual continous addr) , vmalloc(alloc virtual continous addr,
				but more performance cost , so used only needed) , kfree , vfree
		on all these funcs , we have flags ,gft_t ,  used to specify which context and mem zone the allocate func use,
			often used ones are GFP_KERNEL(used in process context and can block) ,GFP_ATOMIC(used in interrupt or
				spin lock context , not sleep)
			remember vfree  kfree can both sleep!!
		
	
	
SLAB LAYER:
	in kernel , we use slab to manage the allocation of memory , by groups of struct type , so as to make alloc and memory management 
		more efficient and less fragments.
	use struct kmem_cache  to represent a cache( a type of data that will be allocated) , in which we have lots of slabs ,which have 
		lots of same data structs with physical addr continously.
		
		details when alloca: ,first , used kmem_cache_create to create a cache ,
				then , kmem_cache_alloc , kmem_cache_free to get the space to store structs and free
				then kmem_cache_destroy at last to free the cache whole.
	
	about store vars on kernel stack:
		the stack is small , often one or two page per process .when one , the interrupt will get a unique page per processor.
			use alloca instead of stack to allocate big data structs.

	when use high memory with alloca_pages() , we have no virtual addr , so we use kmap , kunmap to map . (will sleep) , or
		kmap_atomic() ,(will not sleep , so better in interrupt handler)
	

PER CPU VARS
	we can set an array , then use get_cpu() (will disable preempt) , put_cpu() (enable preempt) to get the cpu id , the
	 	store and load the value in the array
	also we have DECLARE_PER_CPU()  DEFINE_PER_CPU , get_cpu_var() set_cpu_var , per_cpu() ,to get the value , 
		the setXXX used to enable preempt , not do anything else

	but at runtime , the modules's per cpu vars must be allocated by  alloc_percpu , then free_percpu
	
	these per cpu vars can reduce locks and cache flushed and improve performance

CHOOSE:
	want contiguous physical addrs , use kmalloc or low level interfaces like alloc_pages , 
	want in interrupt context , use gpf flag  GFP_ATOMIC instead of GFP_KERNEL
	only want virtual , use vmalloc
	want high memory , use alloc_pages , then kmap , 
	want per cpu vars , use those interfaces	


VFS:
	the virtual filesystem used to provide generic interface 
	all filesystem have  struct super block ,  inode , file , dentry
